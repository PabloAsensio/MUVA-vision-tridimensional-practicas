{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2. Visión Estéreo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visión Tridimensional 2020-21.<br>\n",
    "Practica 2.\n",
    "Abril 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este enunciado está en el archivo \"PrácticaStereo2021_Alumnos.ipynb\" o su versión \"pdf\" que puedes encontrar en el Aula Virtual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los objetivos de esta práctica son:\n",
    "* reconstruir puntos de una escena a partir de una serie de correspondencias manuales entre dos imágenes calibradas;\n",
    "* determinar la geometría epipolar de un par de cámaras a partir de sus matrices de proyección;\n",
    "* hacer una reconstrucción densa de la escena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requerimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta práctica es necesario disponer del siguiente software:\n",
    "* Python 3.X \n",
    "* Jupyter http://jupyter.org/.\n",
    "* Las librerías científicas de Python: NumPy, SciPy, y Matplotlib.\n",
    "* La librería OpenCV\n",
    "\n",
    "El material necesario para la práctica se puede descargar del Aula Virtual en la carpeta ``MaterialesPractica`` del tema de visión estéreo. Esta carpeta contiene:\n",
    "* Una serie de pares estéreo en el directorio images;\n",
    "el sufijo del fichero indica si corresponde a la cámara\n",
    "izquierda (_left) o a la derecha (_right). Bajo el\n",
    "directorio ``rectif`` se encuentran varios pares estéreo\n",
    "rectificados.\n",
    "* Un conjunto de funciones auxiliares de ``Python`` en \n",
    "el módulo ``misc.py``. La descripción de las funciones\n",
    "puede consultarse con el comando help o leyendo\n",
    "su código fuente.\n",
    "* El archivo ``cameras.npz`` con las matrices de proyección del par de cámaras con el que se tomaron todas las imágenes con prefijo minoru."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condiciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La fecha límite de entrega será el viernes **10 de mayo de 2021 a las 23:55** (en el Aula Virtual)\n",
    "* La entrega consiste en dos archivos con el código, resultados y respuestas a los ejercicios:\n",
    "  1. Un \"notebook\" de Jupyter con los resultados. Las respuestas a los ejercicios debes introducirlas en tantas celdas de código o texto como creas necesarias, insertadas inmediatamente después de  un enuciado y antes del siguiente.\n",
    "  2. Un documento \"pdf\" generado a partir del fuente de Jupyter, por ejemplo usando el comando ``jupyter nbconvert --execute --to pdf notebook.ipynb``, o simplemente imprimiendo el \"notebook\" desde el navegador en la opción del menú \"File->Print preview\". Asegúrate de que el documento \"pdf\" contiene todos los resultados correctamente ejecutados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los problemas de visión estéreo se supondrá la existencia de un par de cámaras calibradas cuyas matrices de proyección $\\mathbf{P}_i$ vienen dadas\n",
    "por $$\\mathbf{P}_1 = \\mathbf{K}_1\\cdot\\begin{bmatrix}\\mathbf{I} & \\mathbf{0}\\end{bmatrix}\\cdot\n",
    "    \\begin{bmatrix}\n",
    "        \\mathbf{R}_1 & \\mathbf{t}_1\\\\ \\mathbf{0}^T & 1\n",
    "    \\end{bmatrix},$$ $$\\mathbf{P}_2 = \\mathbf{K}_2\\cdot\\begin{bmatrix}\\mathbf{I} & \\mathbf{0}\\end{bmatrix}\\cdot\n",
    "    \\begin{bmatrix}\n",
    "        \\mathbf{R}_2 & \\mathbf{t}_2\\\\ \\mathbf{0}^T & 1\n",
    "    \\end{bmatrix}.$$\n",
    "    \n",
    "En esta práctica se usarán las matrices de proyección de\n",
    "dos cámaras para determinar la posición tridimensional\n",
    "de puntos de una escena. Esto es posible siempre que se\n",
    "conozcan las proyecciones de cada punto en ambas cámaras. Desafortunadamente, esta información no suele estar\n",
    "disponible y para obtenerla es preciso emplear el contenido\n",
    "de las imágenes (sus píxeles) en un proceso de búsqueda\n",
    "conocido como puesta en correspondencia. Conocer las matrices de proyección de las cámaras permite acotar el área\n",
    "de búsqueda gracias a las restricciones que proporciona la\n",
    "geometría epipolar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import numpy.linalg as npla\n",
    "import misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reconstrucción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo un conjunto de correspondencias entre dos\n",
    "imágenes, con matrices de calibración $P_i$ conocidas, es\n",
    "posible llevar a cabo una reconstrucción tridimensional de\n",
    "dichos puntos. En el fichero ``cameras.npz`` se encuentran\n",
    "las matrices de proyección para las dos cámaras. Para cargar\n",
    "este fichero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1=\n",
      " [[-1.59319023e+02  4.10068927e+02 -8.61429776e+01  5.96021124e+04]\n",
      " [ 9.56736123e+01 -6.85256589e+00 -4.31511155e+02  2.98592912e+04]\n",
      " [-8.69896273e-01 -7.51069223e-02 -4.87482742e-01  5.44164509e+02]]\n",
      "P2=\n",
      " [[-1.49296958e+02  4.20482251e+02 -8.03699899e+01  2.66669558e+04]\n",
      " [ 9.61686671e+01 -2.92284678e+00 -4.41950717e+02  3.12991880e+04]\n",
      " [-8.64354364e-01 -5.83462724e-02 -4.99486983e-01  5.42414607e+02]]\n"
     ]
    }
   ],
   "source": [
    "cameras = np.load(\"cameras.npz\")\n",
    "P1 = cameras[\"left\"]\n",
    "P2 = cameras[\"right\"]\n",
    "\n",
    "print(\"P1=\\n\", P1)\n",
    "print(\"P2=\\n\", P2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to show results in a window\n",
    "%matplotlib tk\n",
    "# %matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las imágenes con el prefijo minoru comparten este par de matrices de proyección.\n",
    "\n",
    "Leemos las imágenes y marcamos al menos seis puntos correspondientes en cada una de ella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread(\"images/minoru_cube3_left.jpg\")\n",
    "img2 = cv2.imread(\"images/minoru_cube3_right.jpg\")\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "# plt.figure()\n",
    "# plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt1, pt2 = misc.askpoints(img1,img2)\n",
    "\n",
    "# Recomendación: Una vez marcados la primera vez con toda la precisión \n",
    "# posible, generar dos arrays de numpy aquí, pt1 y pt2, con las\n",
    "# coordenadas marcadas (para no tener que volver a marcarlas). \n",
    "# Una vez colocadas esas variables ¡comentar el código que llama a \n",
    "# miscaskpoints!\n",
    "\n",
    "pt1 = np.array([[202.56676669, 105.1264066 , 207.05588897, 304.76031508,\n",
    "        288.65228807, 202.56676669, 115.16091523],\n",
    "       [  2.73396849,  23.5951838 ,  56.60343586,  25.17957989,\n",
    "        151.40313578, 206.59293323, 150.0828057 ],\n",
    "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
    "          1.        ,   1.        ,   1.        ]])\n",
    "\n",
    "pt2 = np.array([[117.22543136,   5.26144036,  82.89684921, 205.42348087,\n",
    "        200.93435859,  96.89234809,  25.59452363],\n",
    "       [  5.63869467,  27.82024006,  61.88475619,  28.34837209,\n",
    "        158.26885221, 215.30711178, 154.57192798],\n",
    "       [  1.        ,   1.        ,   1.        ,   1.        ,\n",
    "          1.        ,   1.        ,   1.        ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27433076188>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "plt.plot(pt1.T[:,0], pt1.T[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1.** Implementa la función ``M = reconstruct(points1, points2, P1, P2)``\n",
    "que, dados una serie de N puntos 2D ``points1`` de la primera imagen y sus \n",
    "N homólogos ``points2`` de la segunda imagen\n",
    "(ambos en coordenadas homogéneas, 3 x N), y el par de matrices\n",
    "de proyección P1 y P2 de la primera y la segunda cámara\n",
    "respectivamente, calcule la reconstrucción tridimensional\n",
    "de cada punto. De ese modo, si ``points1`` y\n",
    "``points2`` son 3 × N , la matriz resultante M debe ser 4 × N.\n",
    "\n",
    "El tipo de reconstrucción debe ser algebraico, no geométrico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(points1, points2, P1, P2):\n",
    "# \"\"\"Reconstruct a set of points projected on two images.\"\"\"\n",
    "\n",
    "# Transform homog to cartesian co-ordinates\n",
    "    points1 = points1.T\n",
    "    points2 = points2.T\n",
    "\n",
    "    p11 = P1[0]\n",
    "    p12 = P1[1]\n",
    "    p13 = P1[2]\n",
    "\n",
    "    p21 = P2[0]\n",
    "    p22 = P2[1]\n",
    "    p23 = P2[2]\n",
    "    \n",
    "    Ms = []\n",
    "\n",
    "    for p1, p2 in zip(points1, points2):\n",
    "        i1, j1 = p1[1], p1[0]\n",
    "        i2, j2 = p2[1], p2[0]\n",
    "        \n",
    "        A = np.array([\n",
    "            [(p11 - j1*p13)],\n",
    "            [(p12 - i1*p13)],\n",
    "            [(p21 - j2*p23)],\n",
    "            [(p22 - i2*p23)]\n",
    "        ])       \n",
    "        \n",
    "        # reshape (4,1,4) -> (4,4)\n",
    "        A = np.squeeze(A)\n",
    "        \n",
    "        A_ = A[:,0:3]\n",
    "        b = -A[:,3].T\n",
    "        \n",
    "        # build coefficient matrix and compute reconstruction by least-squares.\n",
    "        # Useful functions are npla.lstsq() and npla.pinv()\n",
    "        M = npla.lstsq(A_, b, rcond=None)[0]\n",
    "        \n",
    "        Ms.append(M)\n",
    "\n",
    "    # Added to make them homog\n",
    "    ones = np.ones((1,len(points1)))\n",
    "    \n",
    "    M = np.array(Ms).T\n",
    "    M = np.vstack((M, ones))\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruye los puntos marcados y pinta su estructura 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes3D:xlabel='X', ylabel='Y'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reconstruct\n",
    "mM = reconstruct(pt1, pt2, P1, P2)\n",
    "# convert from homog to cartesian\n",
    "\n",
    "# plot 3D\n",
    "plt.figure()\n",
    "misc.plot3D(mM[0,:],mM[1,:],mM[2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2.**  Elige un par estéreo de las imágenes del conjunto \"building\" de la práctica de calibración y realiza una reconstrucción de un conjunto de puntos de dicho edificio estableciendo las correspondencias a mano.\n",
    "\n",
    "En este caso tenemos la cámara calibrada dado que las imágenes las hemos capturado con la misma cámara que en la práctrica de calibración. Nos faltarían la posición relativa entre una cámara y la otra. Utilizar algunas funciones de OpenCV en el módulo de calibración de calib3d puede ser de gran ayuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De la práctica de calibración\n",
    "K_building = np.array([\n",
    "    [3.20506009e+03, 0.00000000e+00, 1.97863570e+03],\n",
    "    [0.00000000e+00, 3.20506009e+03, 1.45074623e+03],\n",
    "    [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]\n",
    "])\n",
    "# No usar los parámetros de distorsión radial!!\n",
    "\n",
    "img1_building = cv2.imread(\"building/build_001.jpg\") # izda\n",
    "img2_building = cv2.imread(\"building/build_002.jpg\") # dcha\n",
    "\n",
    "# Get points in both images\n",
    "# plt.figure()\n",
    "# plt.imshow(cv2.cvtColor(img1_building, cv2.COLOR_BGR2RGB))\n",
    "# plt.figure()\n",
    "# plt.imshow(cv2.cvtColor(img2_building, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt1_building, pt2_building = misc.askpoints(img1_building,img2_building)\n",
    "\n",
    "pt2_building = np.array([[1.78733256e+03, 9.35983721e+02, 8.18104651e+02, 1.81025349e+03, 1.78733256e+03,\n",
    "        2.77948140e+03, 2.94647674e+03, 3.12984419e+03, 3.18878372e+03,\n",
    "        1.80043023e+03, 1.79388140e+03, 6.41286047e+02,\n",
    "        5.98718605e+02],\n",
    "       [1.11638419e+02, 8.41833767e+02, 2.06646633e+03, 1.75212214e+03, 1.11638419e+02,\n",
    "        7.82894233e+02, 2.00097795e+03, 2.09266167e+03, 2.44302447e+03,\n",
    "        2.40700586e+03, 1.83398260e+03, 2.17124772e+03,\n",
    "        2.52488493e+03],\n",
    "       [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
    "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
    "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
    "        1.00000000e+00, 1.00000000e+00]]).astype(np.float64)\n",
    "\n",
    "pt1_building = np.array([[1.97337907e+03,1.13840233e+03, 1.01724884e+03, 1.99302558e+03, 1.97337907e+03,\n",
    "        2.98190000e+03, 3.14889535e+03, 3.34208605e+03, 3.40430000e+03,\n",
    "        1.98320233e+03, 1.97665349e+03, 8.46979070e+02,\n",
    "        8.04411628e+02],\n",
    "       [1.31284930e+02, 8.71303535e+02, 2.07628958e+03, 1.77176865e+03, 1.31284930e+02,\n",
    "        7.82894233e+02, 2.02389888e+03, 2.12213144e+03, 2.48231749e+03,\n",
    "        2.42665237e+03, 1.85035470e+03, 2.18107098e+03,\n",
    "        2.52815935e+03],\n",
    "       [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
    "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
    "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
    "        1.00000000e+00, 1.00000000e+00]]).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.imshow(cv2.cvtColor(img1_building, cv2.COLOR_BGR2RGB))\n",
    "# plt.plot(pt1_building.T[:,0], pt1_building.T[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.imshow(cv2.cvtColor(img2_building, cv2.COLOR_BGR2RGB))\n",
    "# plt.plot(pt2_building.T[:,0], pt2_building.T[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1_building:\n",
      " [[3.20506009e+03 0.00000000e+00 1.97863570e+03 0.00000000e+00]\n",
      " [0.00000000e+00 3.20506009e+03 1.45074623e+03 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00]]\n",
      "P2_building:\n",
      " [[ 3.30637548e+03  2.70778914e+01  1.80403919e+03  1.32391017e+03]\n",
      " [ 8.04470310e+01  3.22860332e+03  1.39525041e+03 -2.63974873e+03]\n",
      " [ 5.35275184e-02  1.65337598e-02  9.98429487e-01  2.43123227e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes3D:xlabel='X', ylabel='Y'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encontrar P1_building y P2_building \n",
    "# (matrices de proyección de las dos imágenes seleccionadas)\n",
    "\n",
    "rhs =  np.column_stack( ( np.eye(3) ,np.zeros((3,1)) ) )\n",
    "P1_building = K_building @ rhs\n",
    "                       \n",
    "print(\"P1_building:\\n\", P1_building)\n",
    "\n",
    "\n",
    "# ref: https://docs.opencv.org/4.5.1/d9/d0c/group__calib3d.html#ga13f7e34de8fa516a686a56af1196247f\n",
    "E, _ = cv2.findEssentialMat(pt1_building[:-1].T, pt2_building[:-1].T, K_building)\n",
    "\n",
    "_, R2, t = cv2.decomposeEssentialMat(E)\n",
    "rhs2 =  np.column_stack( ( R2 ,t ) )\n",
    "\n",
    "P2_building = K_building @ rhs2\n",
    "\n",
    "print(\"P2_building:\\n\", P2_building)\n",
    "\n",
    "\n",
    "# # reconstruct\n",
    "mM_building = reconstruct( pt1_building, pt2_building, P1_building , P2_building )\n",
    "\n",
    "# plot 3D\n",
    "plt.figure()\n",
    "misc.plot3D(mM_building[0,:],mM_building[1,:],mM_building[2,:], azim=90, elev=-90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El resto de la práctica lo podemos hacer con los datos de las \n",
    "# dos imágenes de prefijo minoru o las dos seleccionadas del directorio \n",
    "# building \n",
    "\n",
    "usar_par_estereo_building = False\n",
    "\n",
    "if usar_par_estereo_building:\n",
    "    P1 = P1_building\n",
    "    P2 = P2_building\n",
    "    img1 = img1_building\n",
    "    img2 = img2_building\n",
    "    pt1 = pt1_building\n",
    "    pt2 = pt2_building\n",
    "    K = K_building\n",
    "    mM = mM_building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3.**  Reproyecta los resultados de la reconstrucción\n",
    "en las dos cámaras y dibuja las proyecciones sobre las\n",
    "imágenes originales. Pinta también en otro color los puntos seleccionados manualmente. Comprueba si las proyecciones coinciden con los puntos marcados a mano. Comenta los resultados.\n",
    "Para dibujar los puntos puedes usar la función plothom\n",
    "de la práctica anterior o la versión que se distribuye con esta\n",
    "práctica (misc.plothom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proyecto los puntos en ambas cámaras\n",
    "proy1 = P1 @ mM\n",
    "proy2 = P2 @ mM\n",
    "\n",
    "# Pinto con misc.plothom()\n",
    "plt.figure()\n",
    "misc.plothom(proy1,'r.')\n",
    "plt.plot(pt1.T[:,0], pt1.T[:,1], 'bx')\n",
    "plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "misc.plothom(proy2,'r.')\n",
    "plt.plot(pt2.T[:,0], pt2.T[:,1], 'bx')\n",
    "plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Geometría epipolar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La geometría epipolar deriva de las relaciones que aparecen en las proyecciones de una escena sobre un par de\n",
    "cámaras. La matriz fundamental $\\mathbf{F}$, que depende exclusivamente de la configuración de las cámaras y no de la escena\n",
    "que éstas observan, es la representación algebráica de dicha\n",
    "geometría: a partir de ella se pueden calcular los epipolos\n",
    "y las líneas epipolares. La relación entre un par de cámaras\n",
    "$\\mathbf{P}_1$, $\\mathbf{P}_2$ y la matriz fundamental es de n -a- 1 (salvo factor de\n",
    "escala). Es decir, dadas dos cámaras calibradas, sólo tienen\n",
    "una matriz fundamental (excepto un factor de escala); dada\n",
    "una matriz fundamental existen infinitas configuraciones de\n",
    "cámaras posibles asociadas a ella."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Estimación de la matriz fundamental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 4.** Implementa la función ``F = projmat2f(P1, P2)``\n",
    "que, dadas dos matrices de proyección, calcule la matriz\n",
    "fundamental asociada a las mismas. $\\mathbf{F}$ debe ser tal que,\n",
    "si $m_1$ de la imagen 1 y $m_2$ de la imagen 2 están en\n",
    "correspondencia, entonces $m_2^\\top F m_1 = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projmat2f(P1, P2):\n",
    "    \"\"\" Calcula la matriz fundamental a partir de dos matrices de proyeccion\"\"\"\n",
    "    \n",
    "    K1, R1, t1, _, _, _, _= cv2.decomposeProjectionMatrix(P1)\n",
    "    K2, R2, t2, _, _, _, _= cv2.decomposeProjectionMatrix(P2)\n",
    "    \n",
    "    # Correct ts\n",
    "    t1 /= t1[-1] \n",
    "    t2 /= t2[-1] \n",
    "    \n",
    "    t1 = t1[:-1]\n",
    "    t2 = t2[:-1]\n",
    "    \n",
    "    t1 = -R1 @ t1\n",
    "    t2 = -R2 @ t2\n",
    "    \n",
    "    e = -K2 @ R2 @ R1.T @ t1 + K2 @ t2 # epipolo generico\n",
    "    \n",
    "    if e[-1] != 0:\n",
    "        e /= e[-1] # epipolo normalizado\n",
    "    \n",
    "    print(\"Epipolo:\", e.T)\n",
    "    \n",
    "    F = misc.skew(e) @ P2 @ npla.pinv(P1)\n",
    "    \n",
    "    if F[2,2] != 0:\n",
    "        return F / F[2,2]\n",
    "    else:\n",
    "        return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epipolo: [[ 5.44542858e+03 -1.08576575e+04  1.00000000e+00]]\n",
      "F =\n",
      " [[ 6.18328300e-08  3.18506753e-07  3.08058423e-03]\n",
      " [-2.81757724e-07  5.88352229e-09  1.63710279e-03]\n",
      " [-3.39593513e-03 -1.67052450e-03  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# compute Fundamental matrix\n",
    "F = projmat2f(P1, P2)\n",
    "print(\"F =\\n\", F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 5** ¿Cómo es la matriz fundamental de dos cámaras\n",
    "que comparten el mismo centro? (Por ejemplo, dos cámaras\n",
    "que se diferencian sólo por una rotación.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una matriz de zeros.\n",
    "\n",
    "Comprobación para el caso particular: t1 = 0. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "K1 = np.array([[ 4.22521094e+02, 0.0,  1.49785224e+02],\n",
    " [ 0.00000000e+00,  4.23213410e+02, 1.27642797e+02],\n",
    " [ 0.00000000e+00,  0.00000000e+00, 1.00000000e+00]]) \n",
    "\n",
    "K2 = np.array([[4.29684742e+02, 0.0, 1.44655669e+02],\n",
    " [0.00000000e+00, 4.30801382e+02, 1.37795360e+02],\n",
    " [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])\n",
    "\n",
    "\n",
    "R1 = np.array([[-0.06868631,  0.99715456, -0.03106417],\n",
    " [ 0.48842877,  0.00646079, -0.87257985],\n",
    " [-0.86989627, -0.07510692, -0.48748274]])\n",
    "\n",
    "R2 = np.array([[-0.05646744,  0.99822574, -0.01888912],\n",
    " [ 0.49970287,  0.01187786, -0.86611544],\n",
    " [-0.86435436, -0.05834627, -0.49948698]])\n",
    "\n",
    "t1 = np.array([[0],[0],[0]])\n",
    "t2 = t1\n",
    "\n",
    "x = skew( np.squeeze(R2.T @ t2 - R1.T @ t1) ) \n",
    "f1 = npla.inv(K2).T @ R2 @ x\n",
    "f2 = R1.T @ npla.inv(K1)\n",
    "\n",
    "F = f1 @ f2\n",
    "F\n",
    "\n",
    "# array([[0., 0., 0.],\n",
    "#        [0., 0., 0.],\n",
    "#        [0., 0., 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Comprobación de F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los siguientes dos ejercicios vamos a comprobar que la matriz F estimada a partir de P1 y P2 es correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 6.** Comprueba que F es la matriz fundamental asociada a las cámaras ``P1`` y ``P2``. Para ello puedes utilizar el resultado 9.12, que aparece en la página 255 del libro Hartley, Zisserman. \"Multipe View Geometry in Computer Vision.\" (sedond edition). Cambridge University Press, 2003:\n",
    "\n",
    "\"A non zero marix $F$ is a fundamental matrix corresponding to a pair of camera matrices $P$ and $P′$ if and only if $P'^TFP$ is skew symmetric\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F es matriz fundamental\n",
      " [[ 0.       0.       0.00308]\n",
      " [-0.       0.       0.00164]\n",
      " [-0.0034  -0.00167  1.     ]]\n"
     ]
    }
   ],
   "source": [
    "#  skew symmetric: -A=A.T\n",
    "c = P2.T @ F @ P1\n",
    "\n",
    "# redondeo para mitigar el error numerico\n",
    "c = np.around(c, 5)\n",
    "\n",
    "if (-c == c.T).all():\n",
    "    print(\"F es matriz fundamental\\n\", np.around(F, 5))\n",
    "else:\n",
    "    print(\"F no es matriz fundamental\\n\", np.around(F, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se puede comprobar geométricamente la bondad de una matriz F, si  las epipolares con ella estimadas pasan por el homólogo de un punto dado en una de las imágenes.\n",
    "\n",
    "Dada la matriz fundamental $\\mathbf{F}$ entre las cámaras 1 y 2,\n",
    "se puede determinar, para un determinado punto $m_1$ en la\n",
    "imagen de la cámara 1, cuál es la recta epipolar $l_2$ donde se\n",
    "encontrará su homólogo en la cámara 2: $$l_2 = \\mathbf{F} m_1.$$\n",
    "\n",
    "Las siguientes dos funciones sirven para comprobar esta\n",
    "propiedad. En primer lugar, se necesita una función que\n",
    "dibuje rectas expresadas en coordenadas homogéneas, es\n",
    "decir, la versión de plothom para rectas en lugar de puntos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 7.** Implementa la función ``plothline(line)``\n",
    "que, dada una línea expresada en coordenadas homogéneas,\n",
    "la dibuje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plothline(line, axes = None):\n",
    "    \"\"\"Plot a line given its homogeneous coordinates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    line : array_like\n",
    "        Homogeneous coordinates of the line.\n",
    "    axes : AxesSubplot\n",
    "        Axes where the line should be plotted. If not given,\n",
    "        line will be plotted in the active axis.\n",
    "    \"\"\"    \n",
    "    if axes == None:\n",
    "        axes = plt.gca()\n",
    "    \n",
    "    [x0, x1, y0, y1] = axes.axis()\n",
    "    \n",
    "    #     (x0, y0) ._____________________. (x1, y0)\n",
    "    #              |                     |\n",
    "    #              |                     |\n",
    "    #              |                     |\n",
    "    #              |                     |\n",
    "    #              |                     |\n",
    "    #              |                     |\n",
    "    #     (x0, y1) .---------------------. (x1, y1)\n",
    " \n",
    "    \n",
    "    # POR HACER: Compute the intersection of the line with the image\n",
    "    # borders.\n",
    "\n",
    "    a, b, c = line\n",
    "    \n",
    "    yy0 = -(c+a*x0) / b\n",
    "    yy1 = -(c+a*x1) / b\n",
    "    \n",
    "    plotline = axes.plot([x0, x1], [yy0, yy1], 'r-')\n",
    "    \n",
    "    axes.axis([x0, x1, y0, y1])\n",
    "    return plotline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 8.** Completa la función ``plot_epipolar_lines(image1, image2, F)``\n",
    "que, dadas dos imágenes y la matriz fundamental que\n",
    "las relaciona, pide al usuario puntos en la imagen 1 y\n",
    "dibuje sus correspondientes epipolares en la imagen 2 usando ``plothline``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epipolar_lines(image1, image2, F):\n",
    "    \"\"\"Ask for points in one image and draw the epipolar lines for those points.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image1 : array_like\n",
    "        First image.\n",
    "    image2 : array_like\n",
    "        Second image.\n",
    "    F : array_like\n",
    "        3x3 fundamental matrix from image1 to image2.\n",
    "    \"\"\"\n",
    "    # Prepare the two images.\n",
    "    fig = plt.gcf()\n",
    "    fig.clf()\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n",
    "    ax1.axis('image')\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax2.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB))\n",
    "    ax2.axis('image')\n",
    "    plt.draw()\n",
    "    \n",
    "    ax1.set_xlabel(\"Choose points in left image (or right click to end)\")\n",
    "    point = plt.ginput(1, timeout=-1, show_clicks=False, mouse_pop=2, mouse_stop=3)\n",
    "    while len(point) != 0:\n",
    "        # point has the coordinates of the selected point in the first image.\n",
    "        point = np.hstack([np.array(point[0]), 1])\n",
    "        ax1.plot(point[0], point[1], '.r')\n",
    "        \n",
    "        # POR HACER: Determine the epipolar line.\n",
    "        line = F @ point\n",
    "        \n",
    "        # Plot the epipolar line with plothline (the parameter 'axes' should be ax2).\n",
    "        plothline(line, axes=ax2)\n",
    "        plt.draw()\n",
    "        \n",
    "        # Ask for a new point.\n",
    "        point = plt.ginput(1, timeout=-1, show_clicks=False, mouse_pop=2, mouse_stop=3)\n",
    "    \n",
    "    ax1.set_xlabel('')\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza esta función con un par de imágenes llamándola\n",
    "de dos formas diferentes: seleccionando puntos en la imagen\n",
    "izquierda y dibujando las epipolares en la imagen derecha\n",
    "y viceversa. Comprueba en ambos casos que las epipolares\n",
    "siempre pasan por el punto de la segunda imagen correspondiente al seleccionado en la primera. Esto confirmara la corrección de la matriz F.\n",
    "\n",
    "Añade dos figuras una que muestre la selección de puntos en\n",
    "la imagen izquierda y las rectas correspondientes en la\n",
    "imagen derecha, y otra que lo haga al revés. Indica para\n",
    "ambos casos qué matriz fundamental has usado al llamar a\n",
    "``plot_epipolar_lines``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_epipolar_lines(img1, img2, F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Rectificación de imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayoría de algoritmos de puesta en correspondencia,\n",
    "incluyendo el que se va a implementar en esta práctica,\n",
    "requieren que las imágenes de entrada estén rectificadas.\n",
    "\n",
    "Dos imágenes están rectificadas si sus correspondientes epipolares están alineadas horizontalmente. La rectificación de\n",
    "imágenes facilita enormemente los algoritmos de puesta en\n",
    "correspondencia, que pasan de ser problemas de búsqueda\n",
    "bidimensional a problemas de búsqueda unidimensional\n",
    "sobre filas de píxeles de las imágenes. En el material de\n",
    "la práctica se han incluido dos funciones que rectifican\n",
    "(mediante un método lineal) dos imágenes. La función\n",
    "``H1, H2 = misc.projmat2rectify(P1, P2, imsize)``\n",
    "devuelve, dadas las dos matrices de proyección y el tamaño de las imágenes en formato (filas,columnas), las\n",
    "homografías que rectifican, respectivamente, la imagen 1\n",
    "y la imagen 2. La función ``projmat2rectify`` hace uso\n",
    "de ``projmat2f``, por lo que\n",
    "es necesario que esta función esté disponible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 9.** Se tienen dos imágenes no rectificadas ``im1`` e\n",
    "``im2``, y su matriz fundamental asociada $\\mathbf{F}$ . Con el procedimiento explicado, se encuentran un par de homografías $\\mathbf{H}_1$ y $\\mathbf{H}_2$ que dan lugar a las imágenes rectificadas ``O1`` y ``O2``. ¿Cuál es la matriz fundamental $\\mathbf{F}′$ asociada a estas dos imágenes? ¿Por qué?\n",
    "\n",
    "Nota: F ′ depende exclusivamente de F , H1 y H2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F' = H2.T @ F @ pinv(H1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 10.** Rectifica el par de imágenes estéreo ``img1`` e ``img2`` implementando el algoritmo de ``Fusiello, Trucco y Verri`` visto en clase.\n",
    "\n",
    "Para este ejercicio puede ser útil la función ``cv2.decomposeProjectionMatrix``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projmat2rectify_fusiello(P1, P2):\n",
    "    \"\"\"\n",
    "    Determine the transformation for the epipolar rectification.\n",
    "    \n",
    "    Given the projection matrices of an stereo pair and the size\n",
    "    of the images from the cameras, this function returns a pair\n",
    "    of linear transformations (i.e., homographies) which rectify\n",
    "    the images so that the epipolar lines correspond to the scanlines.\n",
    "    \n",
    "    \"A Compact Algorithm for Rectification of Stereo Pairs\"\n",
    "    Andrea Fusiello, Emanuele Trucco, Alessandro Verri.\n",
    "    IJCV 2000\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    P1, P2 : ndarray\n",
    "        Projection matrices of the cameras.\n",
    "    imsize : tuple\n",
    "        The size of the image (height, width)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ref: https://www.researchgate.net/publication/2471375_Rectification_With_Unconstrained_Stereo_Geometry\n",
    "    \n",
    "    K1, R1, t1, _, _, _, _ = cv2.decomposeProjectionMatrix(P1)\n",
    "    K2, R2, t2, _, _, _, _ = cv2.decomposeProjectionMatrix(P2)\n",
    "    \n",
    "    # Correct ts\n",
    "    t1 /= t1[-1] \n",
    "    t2 /= t2[-1] \n",
    "    \n",
    "    t1 = t1[:-1]\n",
    "    t2 = t2[:-1]\n",
    "    \n",
    "    # Kn\n",
    "    Kn = (K1 + K2) / 2\n",
    "    Kn[0,1] = 0\n",
    "    \n",
    "    # Rn\n",
    "    r1n = (t2-t1).T\n",
    "    r2n = np.cross(R1[-1], r1n)\n",
    "    r3n = np.cross(r1n, r2n)\n",
    "    \n",
    "    Rn = np.vstack((\n",
    "        r1n / np.linalg.norm(r1n), \n",
    "        r2n / np.linalg.norm(r2n), \n",
    "        r3n / np.linalg.norm(r3n)\n",
    "    ))\n",
    "    \n",
    "    # Homografics\n",
    "    H1 = Kn @ Rn @ R1.T @ npla.inv(K1) \n",
    "    H2 = Kn @ Rn @ R2.T @ npla.inv(K2) \n",
    "    \n",
    "    # New proyection matrix\n",
    "    Pn1 = Kn @ np.column_stack((Rn, -Rn @ t1))\n",
    "    Pn2 = Kn @ np.column_stack((Rn, -Rn @ t2))\n",
    "    \n",
    "    return H1.astype(\"float64\"), H2.astype(\"float64\"), Pn1, Pn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1=\n",
      " [[-3.17516863e-01  1.07454140e+00  2.05518462e+02]\n",
      " [-9.90328163e-01 -1.77385218e-01  3.62367404e+03]\n",
      " [-2.15887837e-05  7.30608184e-05  9.06455220e-01]]\n",
      "H2=\n",
      " [[-3.37918354e-01  1.06823026e+00  2.55726017e+02]\n",
      " [-1.01208197e+00 -1.85511719e-01  3.49699737e+03]\n",
      " [-3.78751423e-05  6.80081045e-05  9.44655088e-01]]\n"
     ]
    }
   ],
   "source": [
    "H1, H2, Pn1, Pn2 = projmat2rectify_fusiello( P1, P2 )\n",
    "print(\"H1=\\n\", H1)\n",
    "print(\"H2=\\n\", H2)\n",
    "O1 = cv2.warpPerspective(img1, H1, (img1.shape[1], img1.shape[0]))\n",
    "O2 = cv2.warpPerspective(img2, H2, (img2.shape[1], img2.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'O2')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(cv2.cvtColor(O1, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"O1\")\n",
    "plt.figure()\n",
    "plt.imshow(cv2.cvtColor(O2, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"O2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 11.** Calcula y muestra la matriz fundamental de las imágenes\n",
    "rectificadas. Justifica el resultado obtenido (mira la sección 9.3.1 del libro de Hartley y Zisserman, pág. 248 y 249)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epipolo: [[-1.15474459e+20  1.05945103e+03  1.00000000e+00]]\n",
      "Fr=\n",
      " [[ 0.  0. -0.]\n",
      " [-0. -0. -1.]\n",
      " [ 0.  1. -0.]]\n"
     ]
    }
   ],
   "source": [
    "# In page 249\n",
    "\n",
    "# Fr= np.array([\n",
    "#     [0,0,0],\n",
    "#     [0,0,-1],\n",
    "#     [0,1,0]\n",
    "# ])\n",
    "\n",
    "Fr = projmat2f( Pn1, Pn2 )\n",
    "\n",
    "Fr=Fr/Fr[2,1]\n",
    "print(\"Fr=\\n\", np.around(Fr, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 12.** Usa ``plot_epipolar_lines`` para dibujar varias líneas epiplares de las imágenes rectificadas. Muestra los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epipolar_lines( O1, O2, Fr )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Búsqueda de correspondencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La búsqueda de correspondencias consigue establecer automáticamente las correspondencias de puntos entre dos\n",
    "imágenes (lo que se ha hecho manualmente en el ejercicio 2)\n",
    "haciendo uso de las restricciones que proporciona la geometría epipolar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Cálculo de las medidas de semejanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez rectificadas las dos imágenes de un par estéreo,\n",
    "se pueden buscar las correspondencias. Una matriz de disparidades $\\mathbf{S}$ indica, para cada píxel de la imagen 1\n",
    "rectificada, a cuántos píxeles de diferencia está su correspondencia\n",
    "en la imagen 2 rectificada. En la práctica, para simplificar el problema, podemos considerar que los elementos\n",
    "de $\\mathbf{S}$ son enteros. Para el píxel en la posición $(x, y)$ en la imagen 1, su correspondiente está en $(x + S[y, x], y)$ en la\n",
    "imagen 2. Si $S[y, x] < 0$, la correspondencia está hacia la\n",
    "izquierda; si $S[y, x] > 0$, la correspondencia está hacia la\n",
    "derecha; si $S[y, x] = 0$, las coordenadas de los dos puntos\n",
    "coinciden en ambas imágenes.\n",
    "\n",
    "La búsqueda de correspondencias requiere ser capaz de\n",
    "determinar el parecido visual entre píxeles de dos imágenes.\n",
    "Si los píxeles $m_1$ y $m_2$ son visualmente parecidos, tienen\n",
    "más probabilidad de estar en correspondencia que otros\n",
    "que sean visualmente diferentes. Como la\n",
    "apariencia (el nivel de gris) de un único píxel es propensa\n",
    "al ruido y poco discriminativa, el elemento de puesta en\n",
    "correspondencia será una ventana centrada en el píxel.\n",
    "Dado un píxel $m$ de una imagen, llamaremos vecindad\n",
    "del píxel de radio $K$ al conjunto de píxeles de la imagen que se encuentren dentro de una ventana de tamaño\n",
    "$(2K + 1) × (2K + 1)$ píxeles centrada en $m$ . El número de\n",
    "píxeles de una vecindad de radio $K$ es $N = (2K + 1)^2$.\n",
    "Dadas dos vecindades $w_1$ y $w_2$ de dos píxeles, el parecido\n",
    "visual entre ellas puede calcularse con la suma de *diferencias\n",
    "al cuadrado (SSD)* de cada una de sus componentes\n",
    "$$d_{SSD}(\\mathbf{v}, \\mathbf{w}) = \\sum_{i=1}^N(\\mathbf{v}_i - \\mathbf{w}_i)^2.$$\n",
    "\n",
    "La distancia $d_{SSD}$ es siempre positiva, es pequeña cuando\n",
    "dos ventanas son visualmente parecidas y grande en caso\n",
    "contrario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Estimación de la disparidad sin regularizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La implementación de cálculo de la disparidad para cada píxel pasa por obtener una matriz D con las distancias entre regiones a lo largo de la línea epipolar. El valor en D será un array de tamaño $M × N × L$, donde L es el número de disparidades a explorar,\n",
    "``L = len(disps)``; M y N son, respectivamente, el\n",
    "número de filas y de columnas de las imágenes de entrada.\n",
    "El elemento ``D[y,x,l]`` debe ser la distancia entre la ventana centrada en ``im1[y,x]`` y la ventana centrada en ``im2[y,x + disps[l]]``.\n",
    "\n",
    "``D[y,x,l]`` debe ser muy grande para aquellos valores en los que ``im2[y,x + disps[l]]`` no esté definido, es decir, el índice``(y,x+disps[l])`` se sale de la imagen 2.\n",
    "\n",
    "La matriz D proporciona los costes unitarios $D_i$ de una función de energía sin regularización de la forma $$E(x) = \\sum_{i} D_i(x_i),$$\n",
    "donde $D_i(l)$ viene dado por $D[y,x,l]$, suponiendo que\n",
    "el píxel $i$ tenga coordenadas $(x, y)$. Las variables \n",
    "$x = (x_1 ,\\ldots, x_{NM})$ indican las etiquetas de cada uno de los\n",
    "píxeles. En este caso, las etiquetas son los índices del\n",
    "array ``disps``, que a su vez son las disparidades horizontales.\n",
    "Por eso, a partir de aquí se hablará indistintamente de\n",
    "etiquetas y disparidades. Sólo es necesario recordar que la\n",
    "etiqueta $l$ está asociada a la disparidad ``disps[l]``.\n",
    "\n",
    "\n",
    "Minimizando la energía $x = \\arg\\min_x E(x)$,\n",
    "se obtiene un vector de etiquetas óptimo $x^*$ que indica, para\n",
    "cada píxel, cuál es su disparidad horizontal entre las dos\n",
    "imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Estimación de la disparidad regulariazada: StereoSGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez rectificadas las dos imágenes de un par estéreo, \n",
    "y dadas las medidas de semejanza entre píxeles a lo largo \n",
    "de la línea epipolar para un rango de disparidades dado, se puede \n",
    "modificar la función de energía $E(x)$ para añadir términos de regularización. La idea es que píxeles cercanos tengan una disparidad parecida e intentar evitar los problemas de las zonas de la imagen sin textura.\n",
    "\n",
    "En OpenCV tenemos implementado el algoritmo presentado en el siguiente artículo:\n",
    "\n",
    "``Heiko Hirschmuller. Stereo processing by semiglobal matching and mutual information. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(2):328–341, 2008.``\n",
    "\n",
    "Aunque la función de coste basada en Información Mútua del paper anterior se ha sustituido por la de Birchfield-Tomasi:\n",
    "\n",
    "``Stan Birchfield and Carlo Tomasi. A pixel dissimilarity measure that is insensitive to image sampling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(4):401–406, 1998.``\n",
    "\n",
    "\n",
    "La clase que lo implementa el algoritmo de cálculo de disparidades es ``StereoSGBM``. Normalmente en un par estéreo se pretende reconstruir objetos que se encuentran más cerca, y a una distancia dada de la cámara. Esto nos dará lugar a una disparidad mínima a partir de la cual esos píxeles nos interesan (están más cerca que una distancia detertminada a la cámara). El parámetro ``minDisparity`` del algoritmo es crucial para lograr una estimación de disparidad adecuada. También es imporante el parámetro ``numDisparities`` para obtener una mayor o menor glanuralidad en las disparidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 13.** Calcula las disparidades utilizando las imágenes rectificadas O1 y O2, mediante el algoritmo implementado en StereoSGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-af50bbbe5ea5>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-27-af50bbbe5ea5>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    ... POR HACER ...\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Para las imágenes del edificio de la URJC\n",
    "if usar_par_estereo_building:\n",
    "\n",
    "    ... POR HACER ...\n",
    "\n",
    "    S = ... POR HACER ... # Disparidades para la imagen O1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para las imágenes del cubo \n",
    "if not usar_par_estereo_building:\n",
    "\n",
    "    ... POR HACER ...\n",
    "\n",
    "    S = ... POR HACER ... # Disparidades para la imagen O1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(S,cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 14.** Implementa la función\n",
    "``plot_correspondences(image1, image2, S, H1,H2)``\n",
    "que, dado un par de imágenes sin rectificar, la matriz de\n",
    "disparidades entre las imágenes rectificadas y las homografías que llevan de las imágenes sin rectificar a las imágenes\n",
    "rectificadas, pida al usuario puntos en la primera imagen y\n",
    "dibuje sus correspondencias en la segunda.\n",
    "\n",
    "**Nota:** Hay que tener en cuenta que algunos puntos sobre zonas sin textura tendrán la disparidad incorrecta. Prueba a seleccionar puntos sobre esquinas y otros puntos muy fáciles de emparejar (en ellos la disparidad será aproximadamente correcta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correspondences(image1, image2, S, H1, H2):\n",
    "    \"\"\"\n",
    "    Ask for points in the first image and plot their correspondences in\n",
    "    the second image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image1, image2 : array_like\n",
    "        The images (before rectification)\n",
    "    S : array_like\n",
    "        The matrix of disparities.\n",
    "    H1, H2 : array_like\n",
    "        The homographies which rectify both images.\n",
    "    \"\"\"\n",
    "    # Prepare the two images.\n",
    "    fig = plt.gcf()\n",
    "    fig.clf()\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.imshow(image1)\n",
    "    ax1.axis('image')\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax2.imshow(image2)\n",
    "    ax2.axis('image')\n",
    "    plt.draw()\n",
    "    \n",
    "    ax1.set_xlabel(\"Choose points in left image (or right click to end)\")\n",
    "    point = plt.ginput(1, timeout=-1, show_clicks=False, mouse_pop=2, mouse_stop=3)\n",
    "    while len(point) != 0:\n",
    "        # point has the coordinates of the selected point in the first image.\n",
    "        point = np.c_[np.array(point), 1].T\n",
    "        ax1.plot(point[0,:], point[1,:], '.r')\n",
    "        \n",
    "        # POR HACER: Determine the correspondence of 'point' in the second image.\n",
    "        \n",
    "        # POR HACER: Plot the correspondence with ax2.plot.\n",
    "\n",
    "        ax2.plot( ... POR HACER ... ,'r.')\n",
    "        \n",
    "        plt.draw()\n",
    "        # Ask for a new point.\n",
    "        point = plt.ginput(1, timeout=-1, show_clicks=False, mouse_pop=2, mouse_stop=3)\n",
    "        \n",
    "    ax1.set_xlabel('')\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correspondences( ... POR HACER ... )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
